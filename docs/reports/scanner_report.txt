QUESTION 1. How to build your project from the source code. If you use make or ant, tell me. If not, you must
include the full command(s) you used to build your compiler. (Remember, if I can’t figure out how to compile
your project, your group gets a Fix & Resubmit grade for the project.)

----------------------------------------------------------------------------------------------------------

In order to build our project from source, ensure that you have a GNU compiler installed, navigate to the root
directory of the project, and type 'make' in the command line.  To run the test cases, type 'make test'.

QUESTION 2. A brief description of how your group divided the work. Summarize each group member’s contribution
to the project.

----------------------------------------------------------------------------------------------------------

Rather than a formal division of the work where each member is assigned a certain part of the project, we all
jumped in and contributed as all of our schedules and knowledge of where to go next allowed.  We were working
on the project well before the deadline, and this allowed us a lot of flexibility and discussion in creation of
this first part of the overall project.

Stephen: Created much of the early compiler structure and utility functions
Tyler: Contributed to lexer recognition of some tokens, helped write report
Trevor: Contributed to lexer recognition of some tokens, comical relief
Alex: Contributed to lexer recognition of some tokens

QUESTION 3. A list of any clarifications, assumptions, or additions to the problem assigned. The project
specifications are fairly broad and leave many of the decisions to you. This is an aspect of real software
engineering. If you think major clarifications are necessary, please ask the instructor.

----------------------------------------------------------------------------------------------------------

We were able to derive all information we needed to build the scanner from the language specification, and
therefore have no clarifications or assumptions that need to stated.

QUESTION 4. An overview of your design, an analysis of design alternatives you considered (if any), and key design
decisions. Be sure to document and justify all design decisions you make. Any decision accompanied by
a convincing argument will be accepted. If you realize there are flaws or deficiencies in your design late
in the implementation process, discuss those flaws and how you would have done things differently. Also
include any changes you made to previous parts and why they were necessary.

----------------------------------------------------------------------------------------------------------

Our design for this portion of the project is centered around two main components - the scanner and the lexer.
The scanner's job is to open the source file(s) or string and feed characters to whatever is using it (in our case, the lexer).
It does this through the management of a struct called ScannerContext that it opens when a file or string stream is opened.
The ScannerContext keeps track of the line, column, and whether or not we have hit the end of a line within the open stream.
Through methods such as scanner_next and scanner_backtrack, we are able to feed character by character to the lexer.

The lexer's job is to grab tokens from the scanner and try to return a token (in the form of a struct) to whatever is using it.
Information stored within this token struct include the line number and column it was found on, the token
type (T_BOOLEAN, T_CHAR_LITERAL, etc), and the lexeme of the token.  If it receives a token from the scanner that it can't match to
a type, it is assigned a type of T_ILLEGAL - in other words, a syntax error.

QUESTION 5. A brief description of interesting implementation issues. This should include any non-trivial
algorithms, techniques, and data structures. It should also include any insights you discovered during this
phase of the project. (External sources of implementation help, e.g., websites or books, should be cited here.)

----------------------------------------------------------------------------------------------------------

We didn't run into many issues implementing this portion of the project.  One of the more difficult parts of the
implementation was proper management of FILE streams using fseek in several location throughout the scanner.  We had
a lot of issues getting Cygwin to work - looking back, using Vagrant (vagrantup.com) would've saved us a lot of trouble
in this regard.  As far as data structures go, we used a singly linked list in management of our TokenStream structs.

There were a few different projects we took inspiration from in the work we did:
https://github.com/stephens2424/php/blob/master/lexer/lexers.go
https://chromium.googlesource.com/v8/v8.git/+/master/src/scanner.cc
http://mxr.mozilla.org/mozilla/source/js/narcissus/jsparse.js

QUESTION 6. A list of known problems with your project, and as much as you know about the cause.
• If your project fails a provided test case, but you are unable to fix the problem, describe your
understanding of the problem.
• If you discover problems in your project in your own testing that you are unable to fix, but are not
exposed by the provided test cases, describe the problem as specifically as possible and as much as
you can about its cause. If this causes your project to fail hidden test cases, you may still be able to
receive some credit for considering the problem. If this problem is not revealed by the hidden test
cases, then you will not be penalized for it.

----------------------------------------------------------------------------------------------------------

We were unable to complete our implementation of a hash table for storing strings for the parser. One or two tests do not show the exact expected results, such as the correct line and column of errors; most likely due to our use of peeking. We also needed a hacky workaround for displaying '\n' instead of 0xA. If it didn't work in hidden test cases it would be unsurprising.
